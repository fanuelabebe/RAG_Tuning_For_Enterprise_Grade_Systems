{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "d24cc4e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "# api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "# os.environ['OPENAI_API_KEY'] = api_key\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "a8a637ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import OpenAI\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "594304b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import sys\n",
    "\n",
    "\n",
    "def file_reader(path: str, ) -> str:\n",
    "    fname = os.path.join(path)\n",
    "    with open(fname, 'r') as f:\n",
    "        system_message = f.read()\n",
    "    return system_message\n",
    "            \n",
    "def get_prompt():\n",
    "    prompt_message = file_reader(\"../prompts/prompt_generation_template.txt\")\n",
    "    prompt = str(prompt_message)\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752d20de",
   "metadata": {},
   "source": [
    "## Prompt Generator Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "e7cf4ce6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['context', 'output'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'output'], template='Your task is to formulate exactly 5 questions from given context and provide the answer to each one.\\n\\nEnd each question with a \\'?\\' character and then in a newline write the answer to that question using only \\nthe context provided.\\nThe output MUST BE in a json format. \\n\\nexample:\\n[\\n{{\\n    \"user\": \"What is the name of the company?\",\\n    \"assistant\": \"Google\"\\n}},\\n{{\\n    \"user\": \"What is the name of the CEO?\",\\n    \"assistant\": \"Sundar Pichai\"\\n}}\\n]\\n\\nEach question must start with \"user:\".\\nEach answer must start with \"assistant:\".\\n\\n\\nThe question must satisfy the rules given below:\\n1.The question should make sense to humans even when read without the given context.\\n2.The question should be fully answered from the given context.\\n3.The question should be framed from a part of context that contains important information. It can also be from tables,code,etc.\\n4.The answer to the question should not contain any links.\\n5.The question should be of moderate difficulty.\\n6.The question must be reasonable and must be understood and responded by humans.\\n7.Do no use phrases like \\'provided context\\',etc in the question\\n8.Avoid framing question using word \"and\" that can be decomposed into more than one question.\\n10.The question should expand on what the user input is.\\n11.The Answer to the question should correspond with the output given\\n12.The Answer should align with the output\\n    \\ncontext: {context}\\n\\noutput: {output}'))])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RAG_PROMPT_TEMPLATE = get_prompt()\n",
    "\n",
    "rag_prompt = ChatPromptTemplate.from_template(RAG_PROMPT_TEMPLATE)\n",
    "rag_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571bc8a3",
   "metadata": {},
   "source": [
    "## Embeddings and Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "7f6a138a",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = TextLoader('./week_6_challenge_doc.txt')\n",
    "documents = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size = 1000, chunk_overlap=200, model_name = \"gpt-4-1106-preview\")\n",
    "texts  = text_splitter.split_documents(documents)\n",
    "\n",
    "llm = OpenAI(temperature=0)\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "store = Chroma.from_documents(texts,embeddings, collection_name=\"challenge_document\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbff9f70",
   "metadata": {},
   "source": [
    "## Stringoutput Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "e8db8c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import StrOutputParser\n",
    "\n",
    "str_output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d47bf7f",
   "metadata": {},
   "source": [
    "## Setting up RAG chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "15e933f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "\n",
    "retriever = store.as_retriever()\n",
    "\n",
    "entry_point_and_retriever = RunnableParallel(\n",
    "    {\n",
    "        \"context\" : retriever,\n",
    "        \"output\" : RunnablePassthrough()\n",
    "    }\n",
    ")\n",
    "\n",
    "rag_chain = entry_point_and_retriever | rag_prompt | llm | str_output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "5b8abae9",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Input to ChatPromptTemplate is missing variables {'output'}.  Expected: ['context', 'output'] Received: ['context', 'input']\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_42133/4155173297.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrag_chain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'what is the challenge'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m   1711\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1712\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1713\u001b[0;31m                 input = step.invoke(\n\u001b[0m\u001b[1;32m   1714\u001b[0m                     \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1715\u001b[0m                     \u001b[0;31m# mark each step as a child run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/langchain_core/prompts/base.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mRunnableConfig\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     ) -> PromptValue:\n\u001b[0;32m---> 93\u001b[0;31m         return self._call_with_config(\n\u001b[0m\u001b[1;32m     94\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_format_prompt_with_error_handling\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36m_call_with_config\u001b[0;34m(self, func, input, config, run_type, **kwargs)\u001b[0m\n\u001b[1;32m    953\u001b[0m         )\n\u001b[1;32m    954\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 955\u001b[0;31m             output = call_func_with_variable_args(\n\u001b[0m\u001b[1;32m    956\u001b[0m                 \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    957\u001b[0m             )\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/langchain_core/runnables/config.py\u001b[0m in \u001b[0;36mcall_func_with_variable_args\u001b[0;34m(func, input, config, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    307\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mrun_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0maccepts_run_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"run_manager\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 309\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    310\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/langchain_core/prompts/base.py\u001b[0m in \u001b[0;36m_format_prompt_with_error_handling\u001b[0;34m(self, inner_input)\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mmissing\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdifference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minner_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmissing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m             raise KeyError(\n\u001b[0m\u001b[1;32m     84\u001b[0m                 \u001b[0;34mf\"Input to {self.__class__.__name__} is missing variables {missing}. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m                 \u001b[0;34mf\" Expected: {self.input_variables}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"Input to ChatPromptTemplate is missing variables {'output'}.  Expected: ['context', 'output'] Received: ['context', 'input']\""
     ]
    }
   ],
   "source": [
    "rag_chain.invoke('what is the challenge')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
