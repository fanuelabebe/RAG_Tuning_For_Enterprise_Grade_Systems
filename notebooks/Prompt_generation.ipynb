{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d24cc4e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "# api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "# os.environ['OPENAI_API_KEY'] = api_key\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a8a637ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import OpenAI\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "594304b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import sys\n",
    "\n",
    "\n",
    "def file_reader(path: str, ) -> str:\n",
    "    fname = os.path.join(path)\n",
    "    with open(fname, 'r') as f:\n",
    "        system_message = f.read()\n",
    "    return system_message\n",
    "            \n",
    "def get_prompt():\n",
    "    prompt_message = file_reader(\"../prompts/prompt_generation_template.txt\")\n",
    "    prompt = str(prompt_message)\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752d20de",
   "metadata": {},
   "source": [
    "## Prompt Generator Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e7cf4ce6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['context', 'output'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'output'], template='Your task is to formulate exactly 5 questions from given context and provide the answer to each one.\\n\\nEnd each question with a \\'?\\' character and then in a newline write the answer to that question using only \\nthe context provided.\\nThe output MUST BE in a json format. \\n\\nexample:\\n[\\n{{\\n    \"user\": \"What is the name of the company?\",\\n    \"assistant\": \"Google\"\\n}},\\n{{\\n    \"user\": \"What is the name of the CEO?\",\\n    \"assistant\": \"Sundar Pichai\"\\n}}\\n]\\n\\nEach question must start with \"user:\".\\nEach answer must start with \"assistant:\".\\n\\n\\nThe question must satisfy the rules given below:\\n1.The question should make sense to humans even when read without the given context.\\n2.The question should be fully answered from the given context.\\n3.The question should be framed from a part of context that contains important information. It can also be from tables,code,etc.\\n4.The answer to the question should not contain any links.\\n5.The question should be of moderate difficulty.\\n6.The question must be reasonable and must be understood and responded by humans.\\n7.Do no use phrases like \\'provided context\\',etc in the question\\n8.Avoid framing question using word \"and\" that can be decomposed into more than one question.\\n10.The question should expand on what the user input is.\\n11.The Answer to the question should correspond with the output given\\n12.The Answer should align with the output\\n    \\ncontext: {context}\\n\\noutput: {output}'))])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RAG_PROMPT_TEMPLATE = get_prompt()\n",
    "\n",
    "rag_prompt = ChatPromptTemplate.from_template(RAG_PROMPT_TEMPLATE)\n",
    "rag_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571bc8a3",
   "metadata": {},
   "source": [
    "## Embeddings and Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7f6a138a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = TextLoader('./week_6_challenge_doc.txt')\n",
    "documents = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size = 200, chunk_overlap=50, model_name = \"gpt-4-1106-preview\")\n",
    "texts  = text_splitter.split_documents(documents)\n",
    "\n",
    "llm = OpenAI(temperature=0)\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "store = Chroma.from_documents(texts,embeddings, collection_name=\"challenge_document\")\n",
    "len(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbff9f70",
   "metadata": {},
   "source": [
    "## Stringoutput Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e8db8c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import StrOutputParser\n",
    "\n",
    "str_output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d47bf7f",
   "metadata": {},
   "source": [
    "## Setting up RAG chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "15e933f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "\n",
    "retriever = store.as_retriever()\n",
    "\n",
    "entry_point_and_retriever = RunnableParallel(\n",
    "    {\n",
    "        \"context\" : retriever,\n",
    "        \"output\" : RunnablePassthrough()\n",
    "    }\n",
    ")\n",
    "\n",
    "rag_chain = entry_point_and_retriever | rag_prompt | llm | str_output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5b8abae9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\nuser: What is the goal of the challenge?\\nassistant: The goal of the challenge is to support and reward expertise in different parts of the Machine learning engineering toolbox.\\n\\nuser: What are the fundamental tasks for this week's challenge?\\nassistant: The fundamental tasks for this week's challenge are understanding prompt engineering tools and concepts, familiarizing with language models, developing a plan for prompt generation and testing, and setting up a development environment.\\n\\nuser: What are the key performance indicators for this challenge?\\nassistant: The key performance indicators for this challenge are understanding prompt ranking, understanding prompt matching, and ability to reuse previous knowledge.\\n\\nuser: What are the deliverables for this challenge?\\nassistant: The deliverables for this challenge include a PDF document stored in Google Drive or a published blog link, a link to the code repository on GitHub, and an interim submission by Wednesday 8pm UTC.\\n\\nuser: What are the badges awarded for in this challenge?\\nassistant: The badges are awarded for the best performance in categories such as visualization, quality of code, innovative approach to analysis, writing and presentation, and being the most supportive in the community.\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke('i want to know the goals of the challenge')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
